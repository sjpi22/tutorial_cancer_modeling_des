Lmod Warning:
-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: readline/8.2
(required by: sqlite/3.44.2)
-------------------------------------------------------------------------------



Loading required package: StanHeaders

rstan version 2.32.6 (Stan version 2.32.2)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)

── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::extract() masks rstan::extract()
✖ dplyr::filter()  masks stats::filter()
✖ dplyr::lag()     masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘MASS’

The following object is masked from ‘package:patchwork’:

    area

The following object is masked from ‘package:dplyr’:

    select


Attaching package: ‘bestNormalize’

The following object is masked from ‘package:MASS’:

    boxcox


Attaching package: ‘data.table’

The following objects are masked from ‘package:lubridate’:

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year

The following objects are masked from ‘package:dplyr’:

    between, first, last

The following object is masked from ‘package:purrr’:

    transpose

The following objects are masked from ‘package:reshape2’:

    dcast, melt

Registered S3 method overwritten by 'GGally':
  method from   
  +.gg   ggplot2

Attaching package: ‘assertthat’

The following object is masked from ‘package:tibble’:

    has_name

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
[1] "Running on Sherlock"
[1] "# parallel workers: 20"
<environment: R_GlobalEnv>
72 total combinations of flags 
(sampled to 22 combinations)

Training run 1/22 (flags = list(2L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T07-53-06Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))
2025-04-22 00:53:14.502458: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-04-22 00:53:15.904436: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-04-22 00:53:16.324434: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-04-22 00:53:16.872986: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-04-22 00:53:16.955640: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-04-22 00:53:17.631630: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-04-22 00:53:33.583347: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dense_1[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T07-53-06Z

Training run 2/22 (flags = list(4L, 64L, TRUE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-04-22T07-56-50Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 64)        │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 64)        │       4,160 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_3 (Dropout)   │ (None, 64)        │           0 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       1,040 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         195 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         260 │ dropout_3[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 14,871 (58.09 KB)
 Trainable params: 14,871 (58.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T07-56-50Z

Training run 3/22 (flags = list(4L, 128L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-04-22T08-00-02Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-00-02Z

Training run 4/22 (flags = list(4L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-04-22T08-03-13Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-03-13Z

Training run 5/22 (flags = list(3L, 128L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-04-22T08-05-09Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-05-09Z

Training run 6/22 (flags = list(1L, 64L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-04-22T08-07-04Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dropout[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-07-04Z

Training run 7/22 (flags = list(3L, 64L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-04-22T08-10-03Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-10-03Z

Training run 8/22 (flags = list(3L, 32L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-13-07Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 32)        │       1,056 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │         528 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │          99 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         132 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 3,319 (12.96 KB)
 Trainable params: 3,319 (12.96 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-13-07Z

Training run 9/22 (flags = list(3L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-04-22T08-16-09Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-16-09Z

Training run 10/22 (flags = list(3L, 64L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-04-22T08-18-40Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-18-40Z

Training run 11/22 (flags = list(2L, 64L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-21-45Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dropout_1[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-21-45Z

Training run 12/22 (flags = list(2L, 32L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-24-50Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 32)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 32)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │         528 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │          99 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         132 │ dropout_1[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,263 (8.84 KB)
 Trainable params: 2,263 (8.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-24-50Z

Training run 13/22 (flags = list(4L, 32L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-27-54Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 32)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 32)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 32)        │       1,056 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 32)        │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 32)        │       1,056 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_3 (Dropout)   │ (None, 32)        │           0 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │         528 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │          99 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         132 │ dropout_3[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,375 (17.09 KB)
 Trainable params: 4,375 (17.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-27-54Z

Training run 14/22 (flags = list(3L, 128L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-31-02Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-31-02Z

Training run 15/22 (flags = list(1L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-34-16Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-34-16Z

Training run 16/22 (flags = list(2L, 64L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-04-22T08-37-16Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dense_1[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-37-16Z

Training run 17/22 (flags = list(3L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-04-22T08-40-20Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-04-22T08-40-20Z

Training run 18/22 (flags = list(1L, 128L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-04-22T08-43-28Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       2,064 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         387 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         516 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,759 (18.59 KB)
 Trainable params: 4,759 (18.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 
