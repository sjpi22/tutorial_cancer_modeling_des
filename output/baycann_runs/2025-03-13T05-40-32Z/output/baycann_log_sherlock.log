── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::between()     masks data.table::between()
✖ dplyr::filter()      masks stats::filter()
✖ dplyr::first()       masks data.table::first()
✖ lubridate::hour()    masks data.table::hour()
✖ lubridate::isoweek() masks data.table::isoweek()
✖ dplyr::lag()         masks stats::lag()
✖ dplyr::last()        masks data.table::last()
✖ lubridate::mday()    masks data.table::mday()
✖ lubridate::minute()  masks data.table::minute()
✖ lubridate::month()   masks data.table::month()
✖ lubridate::quarter() masks data.table::quarter()
✖ lubridate::second()  masks data.table::second()
✖ purrr::transpose()   masks data.table::transpose()
✖ lubridate::wday()    masks data.table::wday()
✖ lubridate::week()    masks data.table::week()
✖ lubridate::yday()    masks data.table::yday()
✖ lubridate::year()    masks data.table::year()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘assertthat’

The following object is masked from ‘package:tibble’:

    has_name

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
Warning message:
In readLines(file, warn = readLines.warn) :
  incomplete final line found on 'configs/configs.yaml'
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
[1] "Running on Sherlock"
[1] "# parallel workers: 4"
    user   system  elapsed 
2779.078    9.353  704.184 
[1] TRUE
[1] TRUE
Lmod Warning:
-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: readline/8.2
(required by: sqlite/3.44.2)
-------------------------------------------------------------------------------



Loading required package: StanHeaders

rstan version 2.32.6 (Stan version 2.32.2)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)

── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::extract() masks rstan::extract()
✖ dplyr::filter()  masks stats::filter()
✖ dplyr::lag()     masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘MASS’

The following object is masked from ‘package:patchwork’:

    area

The following object is masked from ‘package:dplyr’:

    select


Attaching package: ‘bestNormalize’

The following object is masked from ‘package:MASS’:

    boxcox


Attaching package: ‘data.table’

The following objects are masked from ‘package:lubridate’:

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year

The following objects are masked from ‘package:dplyr’:

    between, first, last

The following object is masked from ‘package:purrr’:

    transpose

The following objects are masked from ‘package:reshape2’:

    dcast, melt

Registered S3 method overwritten by 'GGally':
  method from   
  +.gg   ggplot2

Attaching package: ‘assertthat’

The following object is masked from ‘package:tibble’:

    has_name

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
Warning message:
In readLines(file, warn = readLines.warn) :
  incomplete final line found on 'configs/configs.yaml'
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
72 total combinations of flags 
(sampled to 22 combinations)

Training run 1/22 (flags = list(2L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-15-09Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))
2025-03-12 22:15:20.231370: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-12 22:15:22.250439: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-12 22:15:22.610522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 22:15:23.500687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-12 22:15:23.582649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-12 22:15:24.441285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-12 22:15:47.320214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dense_1[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-15-09Z

Training run 2/22 (flags = list(4L, 64L, TRUE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-18-49Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 64)        │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 64)        │       4,160 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_3 (Dropout)   │ (None, 64)        │           0 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       1,040 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         195 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         260 │ dropout_3[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 14,871 (58.09 KB)
 Trainable params: 14,871 (58.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-18-49Z

Training run 3/22 (flags = list(4L, 128L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-21-34Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-21-34Z

Training run 4/22 (flags = list(4L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-24-17Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-24-17Z

Training run 5/22 (flags = list(3L, 128L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-26-26Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-26-26Z

Training run 6/22 (flags = list(1L, 64L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-28-21Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dropout[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-28-21Z

Training run 7/22 (flags = list(3L, 64L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-30-53Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-30-53Z

Training run 8/22 (flags = list(3L, 32L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-33-28Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 32)        │       1,056 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │         528 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │          99 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         132 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 3,319 (12.96 KB)
 Trainable params: 3,319 (12.96 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-33-28Z

Training run 9/22 (flags = list(3L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-36-01Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-36-01Z

Training run 10/22 (flags = list(3L, 64L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-37-56Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-37-56Z

Training run 11/22 (flags = list(2L, 64L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-40-32Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dropout_1[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 
