── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::between()     masks data.table::between()
✖ dplyr::filter()      masks stats::filter()
✖ dplyr::first()       masks data.table::first()
✖ lubridate::hour()    masks data.table::hour()
✖ lubridate::isoweek() masks data.table::isoweek()
✖ dplyr::lag()         masks stats::lag()
✖ dplyr::last()        masks data.table::last()
✖ lubridate::mday()    masks data.table::mday()
✖ lubridate::minute()  masks data.table::minute()
✖ lubridate::month()   masks data.table::month()
✖ lubridate::quarter() masks data.table::quarter()
✖ lubridate::second()  masks data.table::second()
✖ purrr::transpose()   masks data.table::transpose()
✖ lubridate::wday()    masks data.table::wday()
✖ lubridate::week()    masks data.table::week()
✖ lubridate::yday()    masks data.table::yday()
✖ lubridate::year()    masks data.table::year()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘assertthat’

The following object is masked from ‘package:tibble’:

    has_name

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
Warning message:
In readLines(file, warn = readLines.warn) :
  incomplete final line found on 'configs/configs.yaml'
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
[1] "Running on Sherlock"
[1] "# parallel workers: 4"
    user   system  elapsed 
2779.078    9.353  704.184 
[1] TRUE
[1] TRUE
Lmod Warning:
-------------------------------------------------------------------------------
The following dependent module(s) are not currently loaded: readline/8.2
(required by: sqlite/3.44.2)
-------------------------------------------------------------------------------



Loading required package: StanHeaders

rstan version 2.32.6 (Stan version 2.32.2)

For execution on a local, multicore CPU with excess RAM we recommend calling
options(mc.cores = parallel::detectCores()).
To avoid recompilation of unchanged Stan programs, we recommend calling
rstan_options(auto_write = TRUE)
For within-chain threading using `reduce_sum()` or `map_rect()` Stan functions,
change `threads_per_chain` option:
rstan_options(threads_per_chain = 1)

── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ tidyr::extract() masks rstan::extract()
✖ dplyr::filter()  masks stats::filter()
✖ dplyr::lag()     masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘MASS’

The following object is masked from ‘package:patchwork’:

    area

The following object is masked from ‘package:dplyr’:

    select


Attaching package: ‘bestNormalize’

The following object is masked from ‘package:MASS’:

    boxcox


Attaching package: ‘data.table’

The following objects are masked from ‘package:lubridate’:

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year

The following objects are masked from ‘package:dplyr’:

    between, first, last

The following object is masked from ‘package:purrr’:

    transpose

The following objects are masked from ‘package:reshape2’:

    dcast, melt

Registered S3 method overwritten by 'GGally':
  method from   
  +.gg   ggplot2

Attaching package: ‘assertthat’

The following object is masked from ‘package:tibble’:

    has_name

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
Warning message:
In readLines(file, warn = readLines.warn) :
  incomplete final line found on 'configs/configs.yaml'
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
<environment: R_GlobalEnv>
72 total combinations of flags 
(sampled to 22 combinations)

Training run 1/22 (flags = list(2L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-15-09Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))
2025-03-12 22:15:20.231370: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-12 22:15:22.250439: I external/local_xla/xla/tsl/cuda/cudart_stub.cc:32] Could not find cuda drivers on your machine, GPU will not be used.
2025-03-12 22:15:22.610522: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:485] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered
2025-03-12 22:15:23.500687: E external/local_xla/xla/stream_executor/cuda/cuda_dnn.cc:8454] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered
2025-03-12 22:15:23.582649: E external/local_xla/xla/stream_executor/cuda/cuda_blas.cc:1452] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered
2025-03-12 22:15:24.441285: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.
To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.
2025-03-12 22:15:47.320214: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dense_1[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-15-09Z

Training run 2/22 (flags = list(4L, 64L, TRUE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-18-49Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 64)        │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 64)        │       4,160 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_3 (Dropout)   │ (None, 64)        │           0 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       1,040 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         195 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         260 │ dropout_3[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 14,871 (58.09 KB)
 Trainable params: 14,871 (58.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-18-49Z

Training run 3/22 (flags = list(4L, 128L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-21-34Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-21-34Z

Training run 4/22 (flags = list(4L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-24-17Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 128)       │      16,512 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │       2,064 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │         387 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         516 │ dense_3[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 54,295 (212.09 KB)
 Trainable params: 54,295 (212.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-24-17Z

Training run 5/22 (flags = list(3L, 128L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-26-26Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-26-26Z

Training run 6/22 (flags = list(1L, 64L, TRUE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-28-21Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dropout[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-28-21Z

Training run 7/22 (flags = list(3L, 64L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-30-53Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-30-53Z

Training run 8/22 (flags = list(3L, 32L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-33-28Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 32)        │       1,056 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │         528 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │          99 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         132 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 3,319 (12.96 KB)
 Trainable params: 3,319 (12.96 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-33-28Z

Training run 9/22 (flags = list(3L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T05-36-01Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-36-01Z

Training run 10/22 (flags = list(3L, 64L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-37-56Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-37-56Z

Training run 11/22 (flags = list(2L, 64L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-40-32Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 64)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 64)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dropout_1[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-40-32Z

Training run 12/22 (flags = list(2L, 32L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-43-09Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 32)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 32)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │         528 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │          99 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         132 │ dropout_1[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,263 (8.84 KB)
 Trainable params: 2,263 (8.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-43-09Z

Training run 13/22 (flags = list(4L, 32L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-45-57Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 32)        │         448 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 32)        │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 32)        │       1,056 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 32)        │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 32)        │       1,056 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 32)        │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 32)        │       1,056 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_3 (Dropout)   │ (None, 32)        │           0 │ dense_3[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 16)        │         528 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 3)         │          99 │ dropout_3[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_6 (Dense)       │ (None, 4)         │         132 │ dropout_3[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,375 (17.09 KB)
 Trainable params: 4,375 (17.09 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-45-57Z

Training run 14/22 (flags = list(3L, 128L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-48-54Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-48-54Z

Training run 15/22 (flags = list(1L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-51-46Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-51-46Z

Training run 16/22 (flags = list(2L, 64L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-54-21Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 16)        │       1,040 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 3)         │         195 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 4)         │         260 │ dense_1[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 6,551 (25.59 KB)
 Trainable params: 6,551 (25.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-54-21Z

Training run 17/22 (flags = list(3L, 64L, FALSE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T05-57-06Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-57-06Z

Training run 18/22 (flags = list(1L, 128L, FALSE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T05-59-59Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       2,064 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         387 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         516 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,759 (18.59 KB)
 Trainable params: 4,759 (18.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T05-59-59Z

Training run 19/22 (flags = list(1L, 128L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T06-02-43Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       2,064 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         387 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         516 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,759 (18.59 KB)
 Trainable params: 4,759 (18.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T06-02-43Z

Training run 20/22 (flags = list(1L, 128L, TRUE, 0.25, "sigmoid")) 
Using run directory output/baycann_runs/2025-03-13T06-05-18Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       2,064 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         387 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         516 │ dropout[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 4,759 (18.59 KB)
 Trainable params: 4,759 (18.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T06-05-18Z

Training run 21/22 (flags = list(1L, 64L, FALSE, 0.25, "relu")) 
Using run directory output/baycann_runs/2025-03-13T06-07-56Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 16)        │       1,040 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 3)         │         195 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 4)         │         260 │ dense[0][0]        │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 2,391 (9.34 KB)
 Trainable params: 2,391 (9.34 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T06-07-56Z

Training run 22/22 (flags = list(3L, 128L, TRUE, 0.25, "tanh")) 
Using run directory output/baycann_runs/2025-03-13T06-10-28Z

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 128)       │       1,792 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout (Dropout)     │ (None, 128)       │           0 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 128)       │      16,512 │ dropout[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_1 (Dropout)   │ (None, 128)       │           0 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 128)       │      16,512 │ dropout_1[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dropout_2 (Dropout)   │ (None, 128)       │           0 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       2,064 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         387 │ dropout_2[0][0]    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         516 │ dropout_2[0][0]    │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 37,783 (147.59 KB)
 Trainable params: 37,783 (147.59 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_runs/2025-03-13T06-10-28Z

 elapsed 
57.99753 
Using run directory output/baycann_run_best

> FLAGS <- flags(flag_integer("n_hidden_layers", 2), 
+     flag_integer("n_hidden_nodes", 64), flag_boolean("dropout", 
+         TRUE), flag_numeric .... [TRUNCATED] 

> input <- layer_input(shape = ncol(data_sim_param_train))

> for (i in 1:FLAGS$n_hidden_layers) {
+     if (i == 1) {
+         x <- layer_dense(input, units = FLAGS$n_hidden_nodes, 
+             activation = .... [TRUNCATED] 

> output <- list()

> loss_list <- list()

> loss_weights <- list()

> metrics_list <- list()

> for (i in df_fn_grp_chars$fn_grp) {
+     temp_df_fn_grps <- df_fn_grp_chars[df_fn_grp_chars$fn_grp == 
+         i, ]
+     output[[i]] <- layer_de .... [TRUNCATED] 

> model <- keras_model(inputs = input, outputs = output)

> summary(model)
Model: "functional"
┏━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━┓
┃ Layer (type)          ┃ Output Shape      ┃     Param # ┃ Connected to       ┃
┡━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━┩
│ input_layer           │ (None, 13)        │           0 │ -                  │
│ (InputLayer)          │                   │             │                    │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense (Dense)         │ (None, 64)        │         896 │ input_layer[0][0]  │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_1 (Dense)       │ (None, 64)        │       4,160 │ dense[0][0]        │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_2 (Dense)       │ (None, 64)        │       4,160 │ dense_1[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_3 (Dense)       │ (None, 16)        │       1,040 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_4 (Dense)       │ (None, 3)         │         195 │ dense_2[0][0]      │
├───────────────────────┼───────────────────┼─────────────┼────────────────────┤
│ dense_5 (Dense)       │ (None, 4)         │         260 │ dense_2[0][0]      │
└───────────────────────┴───────────────────┴─────────────┴────────────────────┘
 Total params: 10,711 (41.84 KB)
 Trainable params: 10,711 (41.84 KB)
 Non-trainable params: 0 (0.00 B)

> model %>% compile(loss = loss_list, loss_weights = loss_weights, 
+     optimizer = optimizer_adam(), metrics = metrics_list)

> history <- model %>% fit(x = xtrain_scaled, y = ytrain_scaled_reshape, 
+     epochs = n_epochs, batch_size = n_batch_size, validation_split = 0.2,  .... [TRUNCATED] 

Run completed: output/baycann_run_best

elapsed 
2.77155 
null device 
          1 
[1m1/9[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m5s[0m 646ms/step - dense_3_loss: 3.0416e-04 - dense_3_mae: 0.0070 - dense_4_accuracy: 1.0000 - dense_4_loss: 1.0614 - dense_5_accuracy: 0.9375 - dense_5_loss: 1.3799 - loss: 2.4416[1m9/9[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m1s[0m 3ms/step - dense_3_loss: 3.9542e-04 - dense_3_mae: 0.0077 - dense_4_accuracy: 0.9951 - dense_4_loss: 1.0529 - dense_5_accuracy: 0.9075 - dense_5_loss: 1.3800 - loss: 2.4335  
[1m1/9[0m [32m━━[0m[37m━━━━━━━━━━━━━━━━━━[0m [1m0s[0m 116ms/step[1m9/9[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 9ms/step  [1m9/9[0m [32m━━━━━━━━━━━━━━━━━━━━[0m[37m[0m [1m0s[0m 9ms/step
<environment: R_GlobalEnv>
[1] TRUE

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 1).
Chain 1: 
Chain 1: Gradient evaluation took 0.000354 seconds
Chain 1: 1000 transitions using 10 leapfrog steps per transition would take 3.54 seconds.
Chain 1: Adjust your expectations accordingly!
Chain 1: 
Chain 1: 
Chain 1: Iteration:      1 / 300000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 2).
Chain 2: 
Chain 2: Gradient evaluation took 0.000348 seconds
Chain 2: 1000 transitions using 10 leapfrog steps per transition would take 3.48 seconds.
Chain 2: Adjust your expectations accordingly!
Chain 2: 
Chain 2: 
Chain 2: Iteration:      1 / 300000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 3).
Chain 3: 
Chain 3: Gradient evaluation took 0.000346 seconds
Chain 3: 1000 transitions using 10 leapfrog steps per transition would take 3.46 seconds.
Chain 3: Adjust your expectations accordingly!
Chain 3: 
Chain 3: 
Chain 3: Iteration:      1 / 300000 [  0%]  (Warmup)

SAMPLING FOR MODEL 'anon_model' NOW (CHAIN 4).
Chain 4: 
Chain 4: Gradient evaluation took 0.00022 seconds
Chain 4: 1000 transitions using 10 leapfrog steps per transition would take 2.2 seconds.
Chain 4: Adjust your expectations accordingly!
Chain 4: 
Chain 4: 
Chain 4: Iteration:      1 / 300000 [  0%]  (Warmup)
Chain 1: Iteration:  30000 / 300000 [ 10%]  (Warmup)
Chain 2: Iteration:  30000 / 300000 [ 10%]  (Warmup)
Chain 4: Iteration:  30000 / 300000 [ 10%]  (Warmup)
Chain 3: Iteration:  30000 / 300000 [ 10%]  (Warmup)
Chain 1: Iteration:  60000 / 300000 [ 20%]  (Warmup)
Chain 2: Iteration:  60000 / 300000 [ 20%]  (Warmup)
Chain 3: Iteration:  60000 / 300000 [ 20%]  (Warmup)
Chain 4: Iteration:  60000 / 300000 [ 20%]  (Warmup)
Chain 1: Iteration:  90000 / 300000 [ 30%]  (Warmup)
Chain 2: Iteration:  90000 / 300000 [ 30%]  (Warmup)
Chain 4: Iteration:  90000 / 300000 [ 30%]  (Warmup)
Chain 3: Iteration:  90000 / 300000 [ 30%]  (Warmup)
Chain 1: Iteration: 120000 / 300000 [ 40%]  (Warmup)
Chain 2: Iteration: 120000 / 300000 [ 40%]  (Warmup)
Chain 4: Iteration: 120000 / 300000 [ 40%]  (Warmup)
Chain 3: Iteration: 120000 / 300000 [ 40%]  (Warmup)
Chain 1: Iteration: 150000 / 300000 [ 50%]  (Warmup)
Chain 1: Iteration: 150001 / 300000 [ 50%]  (Sampling)
Chain 2: Iteration: 150000 / 300000 [ 50%]  (Warmup)
Chain 2: Iteration: 150001 / 300000 [ 50%]  (Sampling)
Chain 3: Iteration: 150000 / 300000 [ 50%]  (Warmup)
Chain 3: Iteration: 150001 / 300000 [ 50%]  (Sampling)
Chain 4: Iteration: 150000 / 300000 [ 50%]  (Warmup)
Chain 4: Iteration: 150001 / 300000 [ 50%]  (Sampling)
Chain 3: Iteration: 180000 / 300000 [ 60%]  (Sampling)
Chain 1: Iteration: 180000 / 300000 [ 60%]  (Sampling)
Chain 4: Iteration: 180000 / 300000 [ 60%]  (Sampling)
Chain 2: Iteration: 180000 / 300000 [ 60%]  (Sampling)
Chain 3: Iteration: 210000 / 300000 [ 70%]  (Sampling)
Chain 1: Iteration: 210000 / 300000 [ 70%]  (Sampling)
Chain 4: Iteration: 210000 / 300000 [ 70%]  (Sampling)
Chain 2: Iteration: 210000 / 300000 [ 70%]  (Sampling)
Chain 3: Iteration: 240000 / 300000 [ 80%]  (Sampling)
Chain 1: Iteration: 240000 / 300000 [ 80%]  (Sampling)
Chain 4: Iteration: 240000 / 300000 [ 80%]  (Sampling)
Chain 2: Iteration: 240000 / 300000 [ 80%]  (Sampling)
Chain 3: Iteration: 270000 / 300000 [ 90%]  (Sampling)
Chain 1: Iteration: 270000 / 300000 [ 90%]  (Sampling)
Chain 4: Iteration: 270000 / 300000 [ 90%]  (Sampling)
Chain 2: Iteration: 270000 / 300000 [ 90%]  (Sampling)
Chain 3: Iteration: 300000 / 300000 [100%]  (Sampling)
Chain 3: 
Chain 3:  Elapsed Time: 225.139 seconds (Warm-up)
Chain 3:                345.642 seconds (Sampling)
Chain 3:                570.781 seconds (Total)
Chain 3: 
Chain 1: Iteration: 300000 / 300000 [100%]  (Sampling)
Chain 1: 
Chain 1:  Elapsed Time: 223.037 seconds (Warm-up)
Chain 1:                388.735 seconds (Sampling)
Chain 1:                611.772 seconds (Total)
Chain 1: 
Chain 4: Iteration: 300000 / 300000 [100%]  (Sampling)
Chain 4: 
Chain 4:  Elapsed Time: 225.163 seconds (Warm-up)
Chain 4:                401.044 seconds (Sampling)
Chain 4:                626.207 seconds (Total)
Chain 4: 
Chain 2: Iteration: 300000 / 300000 [100%]  (Sampling)
Chain 2: 
Chain 2:  Elapsed Time: 224.945 seconds (Warm-up)
Chain 2:                411.26 seconds (Sampling)
Chain 2:                636.205 seconds (Total)
Chain 2: 
ci_level: 0.8 (80% intervals)
outer_level: 0.95 (95% intervals)
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Warning message:
Removed 2 rows containing missing values or values outside the scale range
(`geom_bar()`). 
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
`stat_bin()` using `bins = 30`. Pick better value with `binwidth`.
Error in doWithOneRestart(return(expr), restart) : bad error message
Calls: melt ... signal -> withRestarts -> withOneRestart -> doWithOneRestart
Execution halted
── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──
✔ dplyr     1.1.4     ✔ readr     2.1.5
✔ forcats   1.0.0     ✔ stringr   1.5.1
✔ ggplot2   3.5.1     ✔ tibble    3.2.1
✔ lubridate 1.9.3     ✔ tidyr     1.3.1
✔ purrr     1.0.2     
── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──
✖ dplyr::filter() masks stats::filter()
✖ dplyr::lag()    masks stats::lag()
ℹ Use the conflicted package (<http://conflicted.r-lib.org/>) to force all conflicts to become errors
Loading required package: foreach

Attaching package: ‘foreach’

The following objects are masked from ‘package:purrr’:

    accumulate, when

Loading required package: iterators
Loading required package: parallel

Attaching package: ‘data.table’

The following objects are masked from ‘package:lubridate’:

    hour, isoweek, mday, minute, month, quarter, second, wday, week,
    yday, year

The following objects are masked from ‘package:dplyr’:

    between, first, last

The following object is masked from ‘package:purrr’:

    transpose

        R/01_model_inputs_functions.R R/02_decision_model_functions.R
value   ?                             ?                              
visible FALSE                         FALSE                          
        R/03_calibration_general_functions.R R/03a_baycann_functions.R
value   ?                                    ?                        
visible FALSE                                FALSE                    
        R/utils/data_processing.R R/utils/distr_empirical.R
value   ?                         ?                        
visible FALSE                     FALSE                    
        R/utils/epi_functions.R R/utils/query_distr.R R/utils/sum_of_cdfs.R
value   ?                       ?                     ?                    
visible FALSE                   FALSE                 FALSE                
Warning message:
In readLines(file, warn = readLines.warn) :
  incomplete final line found on 'configs/configs.yaml'
<environment: R_GlobalEnv>
